# Vendor Evaluation Framework: 5 Critical Questions

## Core Message

When AI vendors pitch impressive metrics from sports venues or retail deployments, your ability to assess evidence transferability determines whether you invest wisely or join the 80% of organizations that see no tangible enterprise-level impact from AI implementations.

These five questions force vendors to provide verifiable festival-specific evidence or acknowledge the deployment gap.

---

## The 5 Critical Questions

### Question 1: What Festival Deployments Exist with Documented Outcomes?

**Purpose:** Forces vendors to provide verifiable festival examples or admit the absence.

**Red flag responses:**
- "Used by 50+ events" (without specifying stadiums vs festivals)
- NFL and MLB team case studies (sports venues ≠ festivals)
- Generic "major events" language

**What you want:**
- Specific festival names
- Published metrics from festival deployments
- Peer-reviewed case studies or third-party audits

**Why it matters:** Sports venue success doesn't prove festival feasibility. Fifty stadium implementations don't validate a single festival deployment.

---

### Question 2: What Modifications Did You Make Adapting from Stadiums to Festivals?

**Purpose:** Reveals whether vendors understand operational context differences.

**Red flag responses:**
- "Works the same everywhere"
- "No modifications needed"
- Avoiding the question entirely

**What you want:**
- Specific adaptations for temporary infrastructure
- Handling variable attendance patterns (1 event/year vs 40+ games)
- Solutions for integrated systems (security + medical + sanitation coordination)
- Weather/power reliability accommodations

**Why it matters:** If the vendor claims no modifications were needed, they haven't grappled with the operational context mismatch between permanent venues and temporary festivals.

---

### Question 3: What's the Smallest Event You've Successfully Deployed At?

**Purpose:** Exposes scale limitations and data volume requirements.

**Red flag responses:**
- Minimum 50,000+ attendees when your festival is 8,000
- Only stadium-size deployments
- No small-event experience

**What you want:**
- Examples at your festival's size
- Understanding of how system performance scales down
- Honest assessment of minimum viable data volumes

**Why it matters:** Systems optimized for 1.6 billion weekly data points (Legion WFM) may not work at festivals generating only thousands of transactions over three days.

---

### Question 4: Can You Provide Independently Verified Metrics?

**Purpose:** Distinguishes audited evidence from marketing claims.

**Red flag responses:**
- "Our clients achieved..." (vendor-reported only)
- No third-party verification
- Unwillingness to provide audited case studies

**What you want:**
- Forrester-style third-party validation
- Peer-reviewed publications
- Independently audited ROI studies
- University research partnerships (like Roskilde-IBM)

**Why it matters:** Vendor-reported metrics lack the rigor of independent audits. Forrester's validation of Legion WFM's 13x ROI carries more weight than unverified vendor claims.

---

### Question 5: What Are Your Failure Examples and What Did You Learn?

**Purpose:** Tests whether vendors acknowledge deployment reality or claim impossible perfection.

**Red flag responses:**
- "No failures—perfect track record"
- Defensive reactions to the question
- Only success stories

**What you want:**
- Specific failure examples (weather disrupting connectivity, integration delays, accuracy below projections in year one)
- Lessons learned from challenges
- Honest assessment of deployment risks
- How they've improved based on failures

**Why it matters:** Every deployment faces challenges. Vendors sharing failure learnings demonstrate real experience worth trusting. Those claiming perfect records reveal incomplete transparency or insufficient deployments.

---

## Bottom Line

These questions separate evidence-based decisions from marketing hype.

**The pattern of evasion:**
- Leading with sports venue metrics
- Avoiding festival-specific examples
- Claiming universal applicability without modifications
- Only reporting successes, never failures

**The pattern of credibility:**
- Naming specific festivals with published metrics
- Explaining adaptations for festival operational contexts
- Providing third-party validated evidence
- Sharing failure learnings alongside successes

Your evaluation framework protects your organization from expensive mistakes while identifying genuine opportunities where AI analytics already works.

---

## Statistics Referenced

**Deployment Gap:**
- Crowd flow: 50+ festivals (proven)
- Dynamic pricing: 0 verified festival cases
- Staffing optimization: 0 festival deployments
- Food waste AI: 0 outdoor festival deployments

**Scale Thresholds:**
- Legion WFM: 1.6 billion data points/week (incompatible with small festivals)
- Crowd Connected: Custom pricing suggests large-event focus
- Small festivals (<10K): Complete evidence absence

**Validation Standards:**
- Forrester: Third-party audit (Legion 13x ROI)
- Roskilde-IBM: University research partnership
- Peer review: Academic publication standard

**Failure Acknowledgment:**
- Weather connectivity disruptions
- Integration delays
- Year-one accuracy below projections
- Deployment complexity underestimated

---

**Sources:**
- McKinsey (2025): 80% of organizations see no tangible enterprise-level EBIT impact from AI
- Vendor evaluation framework: Section 6 of analytics narrative
- Deployment gaps: Sections 3.1-3.3 documentation
- Validation examples: Legion Forrester study, Roskilde IBM partnership
